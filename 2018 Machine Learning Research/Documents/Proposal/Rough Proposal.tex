\documentclass{article}
\usepackage[numbers]{natbib}
\usepackage{amsmath}
\usepackage[ruled,linesnumbered]{algorithm2e}	
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{float}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor=blue,
	filecolor = blue,
	citecolor=red,
	urlcolor=blue
}
\title{Implementation and Empirical Analysis of Modifications to the Stochastic Gradient Descent Optimizer}
\author{Kenan Karavoussanos}
\begin{document}


\maketitle
\paragraph{Abstract}	
	
	The choice of Optimizer for an Artificial Neural Networks requires multiple considerations, including length of training time, reliance on initial training constants and ability converge to a global minimum rather than a local minimum. This article aims to introduce improvements to the Stochastic Gradient Descent Optimizer and evaluate the resulting performance on various metrics. These Optimizers will be compared in the context of an Artificial Neural Network classifier.The evaluation of each optimizer on a Standard Dataset allows the results obtained to maintain a level of generality such that readers from various fields can make use of the comparison. This comparison is not universally applicable due to the stochastic nature of different datasets and problems. The results contained within this article are specifically applicable to Multiclass classification on balanced datasets.
	

\tableofcontents

\section{Introduction}

	The purpose of an Optimizer in the context of Artificial Neural Networks is to minimize the objective function describing the classification error of the ANN. 
	Stochastic Gradient Descent forms the base for many cutting edge Optimizers such as the Adam Optimizer. Certain strategies are employed in these optimizers to overcome the limitations SGD however these improvements typically come at a price. This paper seeks to elucidate the benefits and trade-offs of each of these strategies with respect to the time to convergence and the level of generalization of the network once it has converged.   


	The specific limitations of SGD are as follows:
	\begin{enumerate}
		\item SGD has slower performance on areas of the surface where the gradient in one dimension is drastically different from the gradient in other dimensions.
		
		\item Stochastic Gradient Descent has a constant learning rate for the entire training epoch which can cause the ANN to overshoot minima and potentially fail to converge.
		
		\item The learning rate is common across all parameters. This is suboptimal as we may want to perform different sized updates to different parameters at certain points.
		
		\item The affinity to stay at a local minimum instead of searching for a better and or global minimum.
		
		\item Stochastic Gradient Descent performs poorly around saddle points.
	\end{enumerate}


	Due to the large amount of computation and calculation required to train an ANN, it is infeasible to trace the training algorithm for more than a few training examples. As such various metrics are used to evaluate the performance of the ANN and its Optimizer. Slow optimizers become prohibitive to training for large datasets so time to convergence has been chosen as a comparison metric. This time to converge shall be measured in two manners: wall clock time as well as number of iterations to convergence. The level of generalization of the network is the most important broad metric in the evaluation of a classifier, thus the accuracy has also been chosen as a metric. 
	
	\paragraph{Significance of problem}
	The current body of research shows the benefits of each new optimizer and how it overcomes specific limitations. However, the trade-offs are rarely specifically studied and as such this paper serves as a means to correct the selection bias in the body of research. 
	
	For specific or niche problem domains and datasets, knowing the performance enhancements and trade offs of each optimizer allows for the reader to select and appropriate Optimizer to fit within various real world constraints.
	
	The evaluation of the each optimizer on a Standard Dataset allows for empirical validation of previous theoretical study on Gradient Descent based Optimizers with specific metrics.
	\paragraph{Application of the problem solution to other domains}
	
	Due to the generality provided by evaluation on a Standard Dataset the problem solution is applicable across many domains. However, this generality also makes the comparison less accurate on niche datasets with imbalanced classes or noisy data as the EMNIST dataset we are using has perfectly balanced classes as well as exceptionally clean data. 
	
	
	\paragraph{Organisation of the rest of the proposal}
	
	The proposal shall consist of a literature survey which analyses the current body of literature that is directly relevant to the current work. Then the Methodology which is split into two sections:
	Methodology-I which describes basic concepts of the work as well as the experimental setup including basic pseudocoded algorithms. Methodology 2 which describes newly developed vectorization of certain optimizers. Followed by the Experimental/Computational Setup describing the hardware and software specification of the experimental environment. The data sources and software engineering principles to be used in the future work are also described.
	
	
\section{Literature Survey}	
The following literature review seeks to critically evaluate the current evaluations of various prolific optimizers used in Artificial Neural Networks. In particular, the benefits of each optimizer in comparison to others as well as the discussed trade-offs of the optimizer. The time to convergence as well as level of generalization will be the comparison metrics discussed.


\paragraph{}~\citet{kingma2014adam} provides an unbiased empirical evaluation of the Adam Optimizer with respect to other optimizers. Adam can be described as a combination of RMSProp, Momentum and a bias-correction term.
Multiple statistics are used for the comparison and shows that Adam is faster to converge than all the other tested optimizers, however it is admitted that Adam tends to over-fit to the training data so regularization and dropout strategies were used to improve generalization. For a Multi-layer Neural Network with dropout stochastic regularization, the training cost versus iteration number is reported for multiple Optimizers. However, the generalization of these optimizers, along our chosen metrics is not reported.

\paragraph{}~\citet{ruder2016overview} provides an overview of various gradient based optimizers and their specific limitations. However the overview is wholly theoretical and does not report any empirical statistics in its analysis.
Provided derivations of each optimizer have been used in the current work for the development of the algorithms used in each  A visualization of some of the discussed optimizers is provided but this does not specifically pertain to the domain of our problem as the visualized objective functions are the Beale Function and a hyperparaboloid. It is important to note the poor performance around saddle points of the optimizers with static learning rates.


\paragraph{}~\citet{darken1991note} discusses learning rate schedules as a solution to a static learning rate as well as introduces the Search-Then-Converge (STC) Method. A major limitation to learning rate schedules discussed is that the learning rate decreases too quickly so convergence is not guaranteed. STC attempts to solve this problem by slowing down this reduction in learning rate and then increasing the reduction once a certain time threshold has been reached. This gives the optimizer time to \textit{search} for a local minimum and then \textit{converge}, hopefully, once it is found. The paper describes that this method is \textit{guaranteed} to converge, however there is no proven guarantee that the optimizer will find a local minimum within the "search" period. This bias warrants that further investigation be reported in the current work to validate the claims of this paper.

\paragraph{}~\citet{duchi2011adaptive} proposes the Adaptive Subgradient Method ,or ADAGRAD.The performance of this method is gauged along two metrics. The online error as well as the test set performance of the resultant classifier after one learning epoch. Various other adaptive gradient methods are tested for comparison and it is shown that ADAGRAD is the best optimizer to use when the training data is sparse. However, for small datasets, ADAGRAD is beat by the AROW algorithm. It is also shown that, on MNIST classification, the error rate for ADAGRAD is 0.4\% greater than that of the Passive-Aggressive Algorithm. The version of ADAGRAD tested in this paper involves the use of Regularised Dual Averaging and thus differs from the ADAGRAD version to be tested in the current work.

\paragraph{}~\citet{smith2017cyclical} proposes  the use of cyclical learning rates as opposed to adaptive learning rates. Cyclical learning rates (CLR) are shown to have less computational cost than other adaptive methods.  The accuracy at 25000 and 70000 training iterations is used as a comparison metric for the different optimizers tested.
The method involves sampling a periodic function which is bounded by a maximum and minimum learning rate. This prevents the learning rate from monotonically decreasing as in ADAGRAD or other learning rate scheduling techniques. This improves the performance of the optimizer around saddle points. Selecting a minimum and maximum learning rate as well as the cycle length of the periodic function is covered by a selection of tests contained within the paper. 

\paragraph{}An interesting result in the paper shows that when using CLR with a gradient acceleration method, The Nesterov Method, it took only 25000 training iterations to achieve accuracy similar to the regular Nesterov Method after 70000 iterations. Similarly, RMSProp with CLR achieves the same improvement. However, when CLR is used in conjunction with the Adam Optimizer, the performance is worse than the regular Adam Optimizer. This suggests some conflict with CLR and either the first-moment term, the bias correction or both in the Adam Optimizer.

 \paragraph{}~\citet{Simard2003BestPF} describes the best practices for Convolutional Neural Networks. In particular using elastic deformation to increase the number of training examples is suggested as a standard for CNN training. It is shown that elastic deformation produces better generalization of the network when compared to affine deformation which in turn is better than no deformation. These methods may potentially be used if the network overfits the training data. This is a computationally expensive method to prevent overfitting when compared to regularization or dropout methods. However, the distortion can be precomputed and reused for various experiments which could potentially save computation time across all experiments in this work. Although this article describes best practices for CNN's this best practice applies to our domain as the dataset is the same.
 
 \paragraph{} ~\citet{lecun1998gradient} compares various types of classifiers on the MNIST database. A MLP with one hidden layer achieved a $4.7\%$ test error with 300 hidden units and $4.5\%$ for a network with 1000 hidden units. This marginal improvement for a large increase in hidden units suggests that the network with 1000 units was overfit i.e poorly generalized, thus the increase in units past a certain threshold between 300 and 1000 increases test error. Alternatively, the increase in number of hidden units could follow a law of diminishing returns and thus while adding units does not increase test error, doing so may not strictly decrease test error. Irrespective of which case holds, it can be seen that arbitrarily adding hidden units does not significantly improve test error and thus other guidelines such as computational time should be considered when deciding on network architecture. A similar pattern holds for a network with two hidden layers.
 
 \paragraph{}~\citet{deepsimpleMLP} evaluate the effect of the number of layers and number of neurons per layer in a Multilayer Perceptron(MLP) Artificial Neural Network when classifying Handwritten Digits. Training examples were distorted using elastic and affine deformations. This distortion was used to artificially increase the number of training examples by distorting all training images in each training epoch. GPU acceleration was used to reduce the training time for each network. The performance of each architecture was measured primarily via the minimum test error achieved at each training epoch. While trialling various architectures the best minimum test error was achieved by a network with 5 hidden layers with 2500,2000,1500,1000,500,10-architecture at $0.32\%$ error. However the training time required to achieve this result was 114.5 hours. A two layer MLP with 1000,500,10-architecture achieved $0.44\%$ error on the test set while only taking 23.4 hours of training time. 
 
 

  
\section{Methodology}
\paragraph{} This section will provide a detailed explanation of the concepts being used as well as the experimental process to be followed in this work. This will be split into two sections:
\begin{itemize}
	\item Methodology-I
	\item Methodology-II
\end{itemize}

\subsection{Methodology-I}
This section provides an overview of the concepts involved in the preliminary work, a detailed explanation of each concept, the equations to be used throughout the preliminary work and the pseudo-code of the learning process with the SGD optimizer. 
\subsubsection{Detailed Concepts}
\paragraph{Artificial Neural Network} An Artificial Neural Network(ANN) is defined as a massively distributed and parallelized processor made up of small computational units called Neurons that acquires knowledge from its environment through a learning process and stores this knowledge as synaptic weights between neurons ~\cite{ANNDefinition}. 

A Neuron takes external data or data from other neurons as input. Each input is multiplied by its respective synaptic weight and then all of these are added up to produce a single value called the activation[need reference] this activation is then passed as input into a non-linear activation function which produces an output which either goes to another Neuron or goes to the output of the network.

Neurons are arranged in layers:
\begin{itemize}
	\item An input layer
	\item An output layer
	\item Zero or more hidden layers. So called as the inputs and outputs to and from the neurons in these layers are typically unobserved.
\end{itemize}
In a feed-forward network, output is passed exclusively to the next layer of neurons. The strength of the connection between two neurons is determined by their synaptic weight. Each neuron in a layer is connected to every neuron in the previous layer and every neuron in the next layer. These values of all the synaptic weights can easily be stored in matrix from called a Weight Matrix. Each layer of a network has its own respective weight matrix. 

The learning process is typically modeled by the backpropogation algorithm combined with an update rule. The backpropogation algorithm is responsible for assigning blame for error in the network's output to the neurons of the network. Once this blame has been assigned, the update rule i.e The Optimizer updates the synaptic weights of each neuron in an attempt to minimize the error function.

\paragraph{Gradient Descent}
Gradient Descent is classified as a first-order iterative optimization algorithm. It works by exploiting the principle that given a multivariable function $J(\underline\theta)$, that is differentiable at a point $\underline{a}$, then $J(\underline\theta)$ decreases fastest along the direction of the negative gradient of $J(\underline\theta)$ at $a$ i.e $-\nabla{J(\underline{a})}$
 
Given a weight matrix $\underline\Theta$, the gradient descent update rule for the weight $\theta_{ji}$ is given by:
\begin{equation}
\theta_ji \leftarrow \theta_{ji} + \Delta\theta_{ji}
\end{equation}

where the weight update, $\Delta\theta_{ji}$, is given by:
\begin{equation}
	\Delta\theta_{ji} := -\alpha\frac{\partial{J}(\theta)}{\partial{\theta_{ji}}}
\end{equation}
 

 $\alpha$ is the learning rate and $\frac{\partial{J}(\theta)}{\partial{\theta_{ji}}}$ is the gradient of the error function with respect to $\theta_{ji}$.
$\frac{\partial{J}(\theta)}{\partial{\theta_{ji}}}$ is calculated using the backpropagation algorithm ~\cite{backpropagationOriginal}.

 
\paragraph{Learning Rate}
The Learning Rate of an ANN can be defined as the rate at which parameters change in a network. That is, given a new update on $\theta_{ji}$, the proportion of $-\frac{\partial{J}(\theta)}{\partial{\theta_{ji}}}$ that is added to the current $\theta_{ji}$ is given by, $\alpha$, our learning rate.
\hfill\newline

The value of the Learning Rate is crucial to the convergence of the network. Too low of a learning rate and the time to convergence becomes exceedingly high. Too high of a learning rate and the network may fail to converge due to continually stepping over the minimum. The Learning Rate can be global to all weights in the network or local to individual or groups of weights. Additionally, the learning rate can be static for the entire learning process or can be adaptive throughout the learning process.  
\paragraph{Momentum}
The Momentum Method is an acceleration method \cite{backpropagationOriginal} used to overcome sudden changes in gradient, that are common around minima, and reduce the time to convergence by setting the next weight update equal to a linear combination of:
\begin{enumerate}
	\item $\frac{\partial{J}(\theta)}{\partial{\theta_{ji}}}$ the gradient of the error function with respect to $\theta_{ji}$
	\item $\Delta\theta_{ji}$ the previous weight update.
\end{enumerate}  
\begin{equation}
\Delta\theta_{ji_{t+1}} := -\alpha\frac{\partial{J}(\theta)}{\partial{\theta_{ji}}} + \beta\Delta\theta_{ji}
\end{equation}


 \hfill \newline
Where $\beta<1$ is the momentum, or exponential decay, constant 
 \hfill \newline
 
Doing this improves the ability of the optimizer to avoid local minima by allowing it to follow the overall trend in gradient rather than the instantaneous gradient. This also speeds up convergence as once the trend in the gradient is found, the optimizer accelerates along this trend towards the minimum.
\hfill\newpage

\subsubsection{Dataset}

The Dataset the ANN will be trained on is the Extended MNIST balanced training set. It is a dataset of 112800 28x28 pixel images of handwritten letters both capitalized and uncapitalized(with letters whose capitalized and uncapitalized versions are indistinguishable reduced to one class), and digits 0-9. In total there are 47 distinct classes of images. The classes are evenly distributed thus the dataset is considered balanced.

The Dataset the ANN will be tested on is the Extended MNIST balanced test set. It is similar to the above training set however there are only 18800 images.

\subsubsection{Architecture}

\paragraph{Input Layer}
The Architecture of our ANN will be a Multilayer Feed Forward Network with 2 hidden layers.
It has been proven that a Neural Network with two hidden layers can approximate any bounded continuous function to arbitrary accuracy \cite{Hornik1989}. The data takes the form of 28x28 pixel images. With each pixel being an individual input we have, $\boldsymbol{N}_i$, the number of inputs equal to 784.

\paragraph{Hidden Layers}
 Due to the fact that all Optimizers will be run on the same network architecture the number of neurons in each hidden layer will be determined beforehand rather than via cross-validation, this prevents the architecture from being tailored to any one optimizer as well as controlling a variable in the experiment.
 
 \citet{panchal2014review} suggest a simple approach to selecting the number of neurons in a network by providing some rules of thumb. Specifically,
\begin{enumerate}
	\item	$\boldsymbol{N}_h\in[\boldsymbol{N}_o, \boldsymbol{N}_i]$
	
	\item$\boldsymbol{N}_h < 2\times\boldsymbol{N}_i$
	
	\item$\boldsymbol{N}_h = \frac{2}{3}\boldsymbol{N}_i + \boldsymbol{N}_o$
\end{enumerate}
 
 Following these guidelines as well as the results in ~\cite{deepsimpleMLP} the following architecture has been proposed:
 
 \begin{equation}
 	\boldsymbol{N}_{h1} = \boldsymbol{N}_{h2} = 600
 \end{equation}

\subsubsection{Performance Metrics}
This section describes the manner in which the various Optimizers shall be compared and evaluated.

Initially, SGD will be tested and will produce the control values for comparison. The following describes the method for attaining said values:
\begin{enumerate}
	\item Run SGD sampling the test accuracy and wall clock time every 10000 training iterations.
	\item Stop at 100000 iterations.
	\item This array of sampling points will serve as control values to compare other optimizers to. 
\end{enumerate}

The use of these control values is described in the following:

\begin{enumerate}
	\item Run Optimizer and sample test accuracy and wall clock time every 10000 training iterations up or until the test accuracy is greater than or equal to the accuracy at the 100000 sample point for SGD.
	\item Sample test error and iteration number at every wall clock time equal to the sampled wall clock times for SGD
\end{enumerate}
This will be reported in the matrices namely:
\begin{itemize}
	\item  \hyperref[sec:IvsTA]{Iteration vs Test Accuracy}
	\item \hyperref[sec:IvsWCT]{Iteration vs Wall clock time}
	\item \hyperref[sec:WCTvsTA]{Wall clock time vs Test Accuracy }
\end{itemize}

\subsubsection{Learning Process Algorithms}
This section provides the pseudocoded algorithms for the learning process with the SGD optimizer, the SGD optimizer with momentum. The pseudocoded algorithms for the adaptive learning rate methods can be found in Methodology-II.

\paragraph{Learning Process With SGD}
The algorithm below shows the full learning process involving the backpropagation algorithm and the SGD optimizer.
\begin{algorithm}
	\SetAlgoLined
	\KwIn{Training Symbols  $\{(\underline{x}_0,\underline{y}_0), ... , (\underline{x}_n,\underline{y}_n)\}$ }
	\KwOut{None}
	
	Initialize all weights in all weight matrices to small non-zero gaussian random numbers.
	
	Initialize all update matrices $\boldsymbol{\Delta\theta}^{(l)}$ to zero matrices.
	
	\ForEach{ $(\underline{\boldsymbol{x}}_i,\underline{y}_i)$ in Symbols}{
		Set $\underline{\boldsymbol{a}}^{(1)} = \underline{\boldsymbol{x}}_i$;
		
		\ForEach{Layer $l\in \{1, ... ,L-1\} $}{
			Set $\underline{\boldsymbol{z}}^{(l+1)} = \boldsymbol\Theta^{(l)}\underline{\boldsymbol{a}}^{(l)}$

			Set $\underline{\boldsymbol{a}}^{(l+1)} = g(\underline{\boldsymbol{z}}^{(l+1)}) $
			}
		Set $\underline{\boldsymbol{\delta}}^{(L)} = \underline{\boldsymbol{a}}^{(L)}-\underline{y}_i$
		
		\ForEach{Layer $l\in \{L-1, ... ,2\}$}{
		
			Set $\underline{\boldsymbol{\delta}}^{(l)} = (\Theta^{(l)})\underline{\boldsymbol{\delta}}^{(l+1)}\times{g^{'}(\underline{\boldsymbol{z}}^{(l)}) }$

}
	\ForEach{Layer $l\in \{1,...,L-1\}$}{
	
		Set $\boldsymbol{\Delta\theta}^{(l)} = -\alpha[\underline{\boldsymbol{\delta}}^{(l+1)}\underline{\boldsymbol{a}}^{(l)^{\boldsymbol{T}}}]$

		Set $\boldsymbol\Theta^{(l)} = \boldsymbol\Theta^{(l)} + \boldsymbol{\Delta\theta}^{(l)}$
}	
			
			
		
	}
\end{algorithm}

\paragraph{SGD with Momentum}

The algorithm shall be modified by adding a proportion, $\beta$, of the previous update matrix i.e $\boldsymbol{\Delta\theta}^{(l)}_i$ to the latest update matrix.

\begin{algorithm}
		\SetAlgoLined
		\KwIn{Training Symbols  $\{(\underline{x}_0,\underline{y}_0), ... , (\underline{x}_n,\underline{y}_n)\}$ }
		\KwOut{None}
		
		Initialize all weights in all weight matrices to small non-zero gaussian random numbers.
		
		Initialize all update matrices $\boldsymbol{\Delta\theta}^{(l)}_0$ to zero matrices.
		
		\ForEach{ $(\underline{\boldsymbol{x}}_i,\underline{y}_i)$ in Symbols}{
			Set $\underline{\boldsymbol{a}}^{(1)} = \underline{\boldsymbol{x}}_i$;
			
			\ForEach{Layer $l\in \{1, ... ,L-1\} $}{
				Set $\underline{\boldsymbol{z}}^{(l+1)} = \boldsymbol\Theta^{(l)}\underline{\boldsymbol{a}}^{(l)}$
				
				Set $\underline{\boldsymbol{a}}^{(l+1)} = g(\underline{\boldsymbol{z}}^{(l+1)}) $
			}
			Set $\underline{\boldsymbol{\delta}}^{(L)} = \underline{\boldsymbol{a}}^{(L)}-\underline{y}_i$
			
			\ForEach{Layer $l\in \{L-1, ... ,2\}$}{
				
				Set $\underline{\boldsymbol{\delta}}^{(l)} = (\Theta^{(l)})\underline{\boldsymbol{\delta}}^{(l+1)}\times{g^{'}(\underline{\boldsymbol{z}}^{(l)}) }$
				
			}
			\ForEach{Layer $l\in \{1,...,L-1\}$}{
				
				Set $\boldsymbol{\Delta\theta}^{(l)}_{i+1} = -\alpha[\underline{\boldsymbol{\delta}}^{(l+1)}\underline{\boldsymbol{a}}^{(l)^{\boldsymbol{T}}}] + \beta\boldsymbol{\Delta\theta}^{(l)}_{i}$
				
				
				Set $\boldsymbol\Theta^{(l)} = \boldsymbol\Theta^{(l)} +\boldsymbol{\Delta\theta}^{(l)}_{i+1} $
			}	
			
			
			
		}
	
\end{algorithm}
\hfill\newpage

\subsubsection{Learning rate schedules}

This section discusses the Search-Then-Converge and Cyclical Learning Rate schedules and provides the governing equations for said schedules.


\paragraph{Search-Then-Converge}
This strategy involves decreasing the learning rate as time passes. The learning rate is given by the following equation:

\begin{equation}
	\alpha = \frac{\alpha_0}{1 + \frac{i}{T}}
\end{equation}
Where $i$ is the index of the current training symbol, $\alpha_0$ and $T$ are selected hyperparameters.
\paragraph{Cyclical Learning Rates}
There are 3 hyperparameters that need selecting for this method:
\begin{itemize}
	\item Stepsize - The number of iterations taken in a half-cycle of the periodic function.
	\item $\alpha_{min}$ - The infimum of our periodic function.
	\item $\alpha_{max}$ - The supremum of our periodic function.
\end{itemize}

It is recommended that the stepsize ranges from 2 to 10 times the number of data points in your training set. It is noted that there is no significant difference in performance along this range.
Thus in our case:

\begin{equation}
stepsize = 2 \times |Training set| = 25300
\end{equation}

The learning rate boundaries are estimated with a test. The test is to run the algorithm for a number of epochs with linearly increasing learning rates. The learning rate versus validation accuracy graph is plotted. $\alpha_{min}$ is the learning rate at which the accuracy starts to increase and $\alpha_{max}$ is the point at which the validation accuracy starts to become jagged or decrease.

The learning rate, using a triangular periodic function, is given by the following equation:
\begin{equation}
	\alpha = \alpha_{min} + (\alpha_{max} - \alpha_{min})\times Max\{0, (1-x)\}
\end{equation}
where 

\begin{equation}
	x = |\frac{iteration}{stepsize} -2\times{(Cycle \space Length)} + 1|
\end{equation}


and \begin{equation}
Cycle \space Length = \lfloor{1 + \frac{iteration}{2\times{stepsize}}}\rfloor
\end{equation}

\subsection{Methodology-II}

\subsubsection{Adaptive Learning Rate Optimizers}
This section proposes various vectorized update rules for the ADAGRAD, RMSProp and Adam Optimizers. 
\paragraph{Adaptive Gradient Method}
The vectorized update rule for ADAGRAD is given by:

\begin{equation}
\boldsymbol{\Delta{\theta}}_{t+1} = -\alpha\boldsymbol{g}_{t+1}\oslash[\epsilon\boldsymbol{I} + \boldsymbol{G}_{t+1}]^{\circ\frac{1}{2}}
\end{equation} 
where 
\begin{itemize}
	\item $\boldsymbol{g}_t$ is the gradient matrix $\underline{\boldsymbol{\delta}}^{(l+1)}\underline{\boldsymbol{a}}^{(l)^{\boldsymbol{T}}}$
	
	\item $\boldsymbol{G}_{t+1}$, termed the normalization matrix is the sum of squares of gradients before time t+1 and is iteratively defined as:
	
	\begin{equation}
	\boldsymbol{G}_{t+1} = \boldsymbol{G}_{t} + \boldsymbol{g}_{t}^{\circ{2}}
	\end{equation} 
	
	\item $\epsilon\boldsymbol{I}$ is a smoothing matrix where all elements are a small number $\epsilon$ to prevent division by zero.
	
	\item $\circ$ is the Hadamard power operator
\end{itemize}



\paragraph{RMSProp}
The main problem with ADAGRAD is that the elements of the normalization matrix are monotonically increasing thus the learning rate is monotonically decreasing at each iteration.
The vectorized update rule is identical to that of ADAGRAD except in this case $\boldsymbol{G}_t$ is now defined as matrix of the the running average of past squared gradients.




\begin{equation}
\boldsymbol{G}_{t+1} = \frac{1}{t+1}[t\boldsymbol{G}_{t} + \boldsymbol{g}_{t}^{\circ{2}}]
\end{equation}
\paragraph{Adam}
Adam combines the benefits of keeping a running average of past squared gradients, the variance, as well as a running average of past gradients, the mean. The first moment term acts as a momentum term as it is a direction preserving exponentially decaying average of previous gradients.

The vectorized update rule for Adam is given by:

\begin{equation}
\boldsymbol{\Delta{\theta}}_{t+1} = -\alpha\boldsymbol{\hat{m}}_{t+1}\oslash{[\epsilon\boldsymbol{I} +\boldsymbol{\hat{v}}_{t+1}^{\circ\frac{1}{2}}]}
\end{equation}
where $\boldsymbol{\hat{m}}_{t+1}$ and $\boldsymbol{\hat{v}}_{t+1}$ are the bias corrected first and second moments of the gradients. This bias correction term is introduced as $\boldsymbol{m}_0$ and $\boldsymbol{v}_0$ are zero matrices and thus the moments are biased towards zero.

$\boldsymbol{\hat{m}}_{t+1}$ is given by the equation:
\begin{equation}
\boldsymbol{\hat{m}}_{t+1} = \frac{1}{1-\beta_1^{t+1}}\boldsymbol{m}_{t+1}
\end{equation}

$\boldsymbol{\hat{v}}_{t+1}$ is given by the equation:
\begin{equation}
\boldsymbol{\hat{v}}_{t+1} = \frac{1}{1-\beta_2^{t+1}}\boldsymbol{v}_{t+1}
\end{equation}

where,
\begin{equation}
\boldsymbol{m}_{t+1} = \beta_1\boldsymbol{m}_{t} + (1-\beta_1)\boldsymbol{g}_{t+1}
\end{equation}

and,
\begin{equation}
\boldsymbol{v}_{t+1} = \beta_2\boldsymbol{v}_{t} + (1-\beta_2)\boldsymbol{g}_{t+1}^{\circ2}
\end{equation}
\subsubsection{Contrast with previous work}

The current work differs from previous work mainly due to the aggregation of various Optimizers that have not been compared in a single source. Additionally, the comparison metrics used in this work extend the comparisons made in previous research. The use of the EMNIST database extends previous comparisons made with the standard MNIST database.

\section{Analytic Results to be used}
This Section shall provide derivations for the newly vectorized update rules for the ADAGRAD, RMSPROP and Adam Optimizers. It will be shown that given the $M \times N$ update matrix, each element, $\boldsymbol{\theta}_{i,j}$,  in the matrix is the single parameter update rule for that optimizer.

\subsection{ADAGRAD and RMSProp}
\subsubsection{Derivation}
Given the vectorized update rule: \hfill\newline

$\boldsymbol{\Delta{\theta}} = -\alpha\boldsymbol{g}\oslash[\epsilon\boldsymbol{I} + \boldsymbol{G}]^{\circ\frac{1}{2}} \hfill\newline\newline\iff \boldsymbol{\Delta{\theta}} =  -\alpha\begin{bmatrix}
g_{11}  & \dots  & g_{1n} \\
\vdots  & \ddots & \vdots \\
g_{m1}  & \dots  & g_{mn}
\end{bmatrix}\oslash\left(\begin{bmatrix}
\epsilon  & \dots  & \epsilon \\

\vdots  & \ddots & \vdots \\
\epsilon  & \dots  & \epsilon
\end{bmatrix} + \begin{bmatrix}
G_{11} &\dots  & G_{1n} \\
\vdots  &\ddots & \vdots \\
G_{m1}  &\dots  & G_{mn}
\end{bmatrix}\right)^{\circ\frac{1}{2}} \hfill\newline\newline\iff  \boldsymbol{\Delta{\theta}} = -\alpha\begin{bmatrix}
g_{11}  & \dots  & g_{1n} \\
\vdots  & \ddots & \vdots \\
g_{m1}  & \dots  & g_{mn}
\end{bmatrix}\oslash\begin{bmatrix}
\sqrt{\epsilon + G_{11}} & \dots  & \sqrt{\epsilon + G_{1n}} \\
\vdots & \ddots & \vdots \\
\sqrt{\epsilon + G_{m1}}  &\dots  & \sqrt{\epsilon + G_{mn}}
\end{bmatrix} \hfill\newline\newline\iff \boldsymbol{\Delta{\theta}} = \begin{bmatrix}
-\alpha\frac{g_{11}}{\sqrt{\epsilon + G_{11}}}  & \dots  & -\alpha\frac{g_{1n}}{\sqrt{\epsilon + G_{1n}}}  \\
\vdots  & \ddots & \vdots \\
-\alpha\frac{g_{m1}}{\sqrt{\epsilon + G_{m1}}}   & \dots  & -\alpha\frac{g_{mn}}{\sqrt{\epsilon + G_{mn}}} 
\end{bmatrix}
$ \hfill\newline\newline
Thus $\boldsymbol{\Delta{\theta}}_{ij} = -\alpha\frac{g_{i,j}}{\sqrt{\epsilon + G_{ij}}} $
\subsubsection{Analysis}
The benefit of using this vectorized equation is that all operations are element-wise algebraic operations. As such, the equation is simple to understand for the reader and programmatic implementation is also simple. The presence of an interchangeable normalization matrix, $\boldsymbol{G}_{t+1}$ , allows various other normalizations (other than Sum of Squared Gradients and the RMS of Gradients) to be easily implemented without having to derive a new vectorized equation. 

\subsection{Adam}
\subsubsection{Derivation}

Given the vectorized update rule: \hfill\newline

$\boldsymbol{\Delta{\theta}} = -\alpha\boldsymbol{\hat{m}}\oslash{[\epsilon\boldsymbol{I} +\boldsymbol{\hat{v}}^{\circ\frac{1}{2}}]}\hfill\newline\newline\iff \boldsymbol{\Delta{\theta}} =  -\alpha\begin{bmatrix}
\hat{m}_{11}  & \dots  & \hat{m}_{1n} \\
\vdots  & \ddots & \vdots \\
\hat{m}_{m1}  & \dots  & \hat{m}_{mn}
\end{bmatrix}\oslash\left(\begin{bmatrix}
\epsilon  & \dots  & \epsilon \\

\vdots  & \ddots & \vdots \\
\epsilon  & \dots  & \epsilon
\end{bmatrix} + \begin{bmatrix}
\hat{v}_{11} &\dots  & \hat{v}_{1n} \\
\vdots  &\ddots & \vdots \\
\hat{v}_{m1}  &\dots  & \hat{v}_{mn}
\end{bmatrix}^{\circ\frac{1}{2}}\right) \hfill\newline\newline\iff  \boldsymbol{\Delta{\theta}} = -\alpha\begin{bmatrix}
\hat{m}_{11}  & \dots  & \hat{m}_{1n} \\
\vdots  & \ddots & \vdots \\
\hat{m}_{m1}  & \dots  & \hat{m}_{mn}
\end{bmatrix}\oslash\begin{bmatrix}
\epsilon + \sqrt{\hat{v}_{11}} & \dots  & \epsilon + \sqrt{\hat{v}_{1n}} \\
\vdots & \ddots & \vdots \\
\epsilon + \sqrt{\hat{v}_{m1}}  &\dots  & \epsilon + \sqrt{\hat{v}_{mn}}
\end{bmatrix} \hfill\newline\newline\iff \boldsymbol{\Delta{\theta}} = \begin{bmatrix}
-\alpha\frac{\hat{m}_{11}}{\epsilon + \sqrt{\hat{v}_{11}}}  & \dots  & -\alpha\frac{\hat{m}_{1n}}{\epsilon + \sqrt{\hat{v}_{1n}}}  \\
\vdots  & \ddots & \vdots \\
-\alpha\frac{\hat{m}_{m1}}{\epsilon + \sqrt{\hat{v}_{m1}}}   & \dots  & -\alpha\frac{\hat{m}_{mn}}{\epsilon + \sqrt{\hat{v}_{mn}}} 
\end{bmatrix}
$ \hfill\newline\newline
Thus $\boldsymbol{\Delta{\theta}}_{ij} = -\alpha\frac{\hat{m}_{ij}}{\epsilon + \sqrt{\hat{v}_{ij}}}  $
\subsubsection{Analysis}
Given the fact that $\epsilon < 1$ and that it is no longer square rooted. It can be deduced that the magnitude of smoothing used in this method is less than that of the other Adaptive Gradient Methods in this work. This is possibly due to the fact that, there is already a bias correction to prevent the $\hat{v}_{ij}$ term from being zero. So having the usual magnitude of smoothing may further bias the term away from zero, which is undesirable.

\section{Experimental Setup and Computational Model}

This section will describe various components of the experimental setup such that the experiment can be repeated and the validity of claims made in future components of this work can be tested.

\subsection{Computational Environment}
\subsubsection{Hardware Specification}
\begin{itemize}
	\item  CPU: Intel(R) Core(TM) i5-3570k CPU @ 3.4-3.8 GHz
	\item  GPU: AMD Radeon HD 5700 Series
	\item  RAM: 4GB
\end{itemize}
\subsection{Computational Model}
\begin{itemize}
	\item Artificial Neural Network
	
\end{itemize}
\subsubsection{Software and Versions}
\begin{itemize}
	\item Operating System: Windows 10
	\item Python v3.6.0
	\item numpy v1.14.3
	\item sympy v1.1.1
\end{itemize}

\subsection{Data Sources}
The data can be obtained from the \href{https://github.com/KenanKarav/CapstoneProject.git}{Github repository} for this work. Within the Datasources folder, you will find the following files:
\begin{itemize}
	\item emnist-balanced-mapping.txt: The mapping of class numbers to their corresponding ASCII value.
	\item emnist-balanced-test.csv.zip: The test dataset.
	\item emnist-balanced-train.csv.zip: The training data.
\end{itemize}
The training and test images consist of 28x28 pixel black and white handwritten letters and digits. Each row of the csv is a distinct image. The first column being the class number and the remaining 784 columns representing each pixel value of the image.

\subsection{Software Engineering practices and Public Accessibility}
The code will follow Object Oriented Design with a focus on modularity to aid in debugging and review of the code by the evaluator of this work. Additionally unit testing will be implemented to try limit the time until a bug is discovered. 

The full source code shall be available at the \href{https://github.com/KenanKarav/CapstoneProject.git}{Github repository} for this work. This repository is publicly accessible however changes to the repository are restricted to the author(s) of this work.
\section{Summary}

Choosing an optimizer for ones problem domain and constraints takes a lot of time seeing as there is no single source of comparison for popular optimizers. This work and future work is intended to wholly and fairly compare the time and generalization performance of various popular gradient based optimizers. This shall provide the reader with an initial source of information to improve the ability of the reader to make an informed choice of optimizer. The results of this comparison are applicable across many problem domains due to the level of generality maintained during experimentation. 

\section{References}	
	
\bibliographystyle{plainnat}
\bibliography{mybib}
\section{Appendices}
\subsection{Tables}
\subsubsection{Iteration versus Accuracy}
\label{sec:IvsTA}
	\begin{table}[H]
		\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|}
			\cline{2-12}
			& \multicolumn{1}{c|}{\multirow{2}{*}{Optimizer}} & \multicolumn{10}{c|}{Iteration Number}                                         \\ \cline{3-12} 
			& \multicolumn{1}{c|}{}                           & 10000 & 20000 & 30000 & 40000 & 50000 & 60000 & 70000 & 80000 & 90000 & 100000 \\ \cline{2-12} 
			& SGD                                             & 30    & 50    & 70    & 80    & 85    & 90    & 92    & 95    & 96    & 99     \\ \cline{2-12} 
			& Momentum                                        & 40    & 60    & 80    & 85    & 88    & 90    & 99    & 99    & 99    & 99     \\ \cline{2-12} 
		\end{tabular}
		\caption{Mock Table of test accuracy of an Optimizer versus Iteration Number}
	\end{table}
\subsubsection{Iteration versus Wall clock time}
\label{sec:IvsWCT}
\begin{table}[H]
	\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|}
		\cline{2-12}
		& \multicolumn{1}{c|}{\multirow{2}{*}{Optimizer}} & \multicolumn{10}{c|}{Iteration Number}                                         \\ \cline{3-12} 
		& \multicolumn{1}{c|}{}                           & 10000 & 20000 & 30000 & 40000 & 50000 & 60000 & 70000 & 80000 & 90000 & 100000 \\ \cline{2-12} 
		& SGD                                             & time1 & time2 & time3 & time4 & time5 & time6 & time7 & time8 & time9 & time10 \\ \cline{2-12} 
		& Momentum                                        & time1 & time2 & time3 & time4 & time5 & time6 & time7 & time8 & time9 & time10 \\ \cline{2-12} 
	\end{tabular}
	\caption{Mock Table of wall clock time at a given iteration number for Optimizers}
\end{table}
\subsubsection{Wall Clock Time versus Test Accuracy}
\begin{table}[H]
	\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|}
		\cline{2-12}
		& \multicolumn{1}{c|}{\multirow{2}{*}{Optimizer}} & \multicolumn{10}{c|}{Wall Clock Time}                                                                                  \\ \cline{3-12} 
		& \multicolumn{1}{c|}{}                           & time1     & time2     & time3     & time4     & time5     & time6     & time7     & time8     & time9     & time10     \\ \cline{2-12} 
		& SGD                                             & accuracy1 & accuracy2 & accuracy3 & accuracy4 & accuracy5 & accuracy6 & accuracy7 & accuracy8 & accuracy9 & accuracy10 \\ \cline{2-12} 
		& Momentum                                        & accuracy1 & accuracy2 & accuracy3 & accuracy4 & accuracy5 & accuracy6 & accuracy7 & accuracy8 & accuracy9 & accuracy10 \\ \cline{2-12} 
	\end{tabular}
	\caption{Mock table showing test accuracy of optimizers versus previously sampled Wall Clock Times}
	\label{sec:WCTvsTA}
\end{table}
\end{document}







